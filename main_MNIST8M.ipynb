{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Making the dataset from transforming orignal MNIST\n",
        "Runtime crash count: 3"
      ],
      "metadata": {
        "id": "WcHUeuPYJG17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO RUN, MAKE SURE YOU HAV ~ 9Gb of RAM AVALABLE TO USE!!!\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Enable mixed precision\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x = np.concatenate((x_train, x_test))\n",
        "y = np.concatenate((y_train, y_test))\n",
        "x = x.astype('float32') / 255.0\n",
        "\n",
        "# Move data to GPU\n",
        "x = tf.constant(x, dtype=tf.float32)\n",
        "y = tf.constant(y, dtype=tf.int32)\n",
        "\n",
        "@tf.function\n",
        "def generate_samples(images, labels, num_samples):\n",
        "    idx1 = tf.random.uniform([num_samples], 0, tf.shape(images)[0], dtype=tf.int32)\n",
        "    idx2 = tf.random.uniform([num_samples], 0, tf.shape(images)[0], dtype=tf.int32)\n",
        "\n",
        "    image1, image2 = tf.gather(images, idx1), tf.gather(images, idx2)\n",
        "    label1, label2 = tf.gather(labels, idx1), tf.gather(labels, idx2)\n",
        "\n",
        "    alpha = tf.random.uniform([num_samples, 1, 1], 0, 1)\n",
        "\n",
        "    new_images = alpha * image1 + (1 - alpha) * image2\n",
        "    new_labels = tf.cast(tf.round(alpha[:, 0, 0] * tf.cast(label1, tf.float32) + (1 - alpha[:, 0, 0]) * tf.cast(label2, tf.float32)), tf.int32)\n",
        "\n",
        "    # Random shift\n",
        "    shift = tf.random.uniform([num_samples, 2], -2, 3, dtype=tf.int32)\n",
        "    new_images = tf.map_fn(lambda x: tf.roll(x[0], x[1], [0, 1]), (new_images, shift), fn_output_signature=tf.float32)\n",
        "\n",
        "    # Random noise\n",
        "    noise = tf.random.normal(tf.shape(new_images), mean=0.0, stddev=0.05)\n",
        "    new_images = new_images + noise\n",
        "\n",
        "    new_images = tf.clip_by_value(new_images, 0, 1)\n",
        "\n",
        "    return new_images, new_labels\n",
        "\n",
        "def generate_and_save_dataset(num_samples, batch_size=8192, base_dir=\"MNIST8M\"):\n",
        "    train_dir = os.path.join(base_dir, \"train\")\n",
        "    test_dir = os.path.join(base_dir, \"test\")\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "    samples_generated = 0\n",
        "\n",
        "    for batch in tqdm(range(num_batches), desc=\"Generating and saving batches\"):\n",
        "        current_batch_size = min(batch_size, num_samples - samples_generated)\n",
        "\n",
        "        new_images, new_labels = generate_samples(x, y, current_batch_size)\n",
        "\n",
        "        # Split into train and test (70-30 split)\n",
        "        split_idx = int(0.7 * current_batch_size)\n",
        "        train_x, test_x = new_images[:split_idx], new_images[split_idx:]\n",
        "        train_y, test_y = new_labels[:split_idx], new_labels[split_idx:]\n",
        "\n",
        "        # Save train data\n",
        "        np.save(os.path.join(train_dir, f\"x_train_{batch}.npy\"), train_x.numpy())\n",
        "        np.save(os.path.join(train_dir, f\"y_train_{batch}.npy\"), train_y.numpy())\n",
        "\n",
        "        # Save test data\n",
        "        np.save(os.path.join(test_dir, f\"x_test_{batch}.npy\"), test_x.numpy())\n",
        "        np.save(os.path.join(test_dir, f\"y_test_{batch}.npy\"), test_y.numpy())\n",
        "\n",
        "        samples_generated += current_batch_size\n",
        "\n",
        "    print(\"Data generation and saving complete.\")\n",
        "    print(f\"Total samples generated: {samples_generated}\")\n",
        "\n",
        "# Generate and save the dataset\n",
        "num_samples = 8000000\n",
        "batch_size = 10000  # Adjust this based on your GPU memory\n",
        "\n",
        "generate_and_save_dataset(num_samples, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG9eQEdeMXiQ",
        "outputId": "0d4c4225-4b67-4f38-b05e-ecd1efcfb890"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available:  []\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating and saving batches: 100%|██████████| 800/800 [10:51<00:00,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data generation and saving complete.\n",
            "Total samples generated: 8000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3Oz0RXo3TEE",
        "outputId": "e8b3bc2c-a4bc-4bfe-fbe2-c845ea597e98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Origial apporach using memmap in numpy took too much timeee so I will use another approch\n",
        "Runtime crash counter: 5.5 (if you count my alloted TPU limit runnning out)..."
      ],
      "metadata": {
        "id": "upJ1sMI7CYYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class LargeDataset:\n",
        "    def __init__(self, data_dir, prefix): # just initializes the dataset\n",
        "        self.data_dir = data_dir\n",
        "        self.files = sorted([f for f in os.listdir(data_dir) if f.startswith(prefix) and f.endswith('.npy')])\n",
        "        self.total_samples = sum(np.load(os.path.join(data_dir, f)).shape[0] for f in self.files)\n",
        "\n",
        "    def __getitem__(self, idx): # makes it so that I can index the datasetl later and not crash my runtime... eventhough it stil does :)\n",
        "        if isinstance(idx, slice):\n",
        "            start, stop, step = idx.indices(self.total_samples)\n",
        "            return self._load_range(start, stop, step)\n",
        "        elif isinstance(idx, int):\n",
        "            if idx < 0:\n",
        "                idx += self.total_samples\n",
        "            if idx < 0 or idx >= self.total_samples:\n",
        "                raise IndexError(\"Index out of range\")\n",
        "            return self._load_single(idx)\n",
        "        else:\n",
        "            raise TypeError(\"Invalid index type\") # error handling for  when someone tries to break it :)... not me!!\n",
        "\n",
        "    def _load_range(self, start, stop, step): # if the index given is a slice i.e. dataset[10:20]\n",
        "        return np.array([self._load_single(i) for i in range(start, stop, step)])\n",
        "\n",
        "    def _load_single(self, idx): # if the index given is not slice i.e. dataset[5]\n",
        "        for file in self.files:\n",
        "            data = np.load(os.path.join(self.data_dir, file))\n",
        "            if idx < len(data):\n",
        "                return data[idx]\n",
        "            idx -= len(data)\n",
        "        raise IndexError(\"Index out of range\")\n",
        "\n",
        "    def __len__(self): # num of total samples\n",
        "        return self.total_samples\n",
        "\n",
        "def load_dataset(base_dir=\"MNIST8M\"): # the fution that ties all together\n",
        "    train_dir = os.path.join(base_dir, \"train\")\n",
        "    test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "    x_train = LargeDataset(train_dir, \"x_train\")\n",
        "    y_train = LargeDataset(train_dir, \"y_train\")\n",
        "    x_test = LargeDataset(test_dir, \"x_test\")\n",
        "    y_test = LargeDataset(test_dir, \"y_test\")\n",
        "\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(f\"Training samples: {len(x_train)}\")\n",
        "    print(f\"Test samples: {len(x_test)}\")\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "# lets see if it works (fingers crossed) :\n",
        "x_train, y_train, x_test, y_test = load_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpVc2SAzCcdg",
        "outputId": "505aee67-bb8a-413b-a273-841d308ebdf4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Training samples: 5600000\n",
            "Test samples: 2400000\n"
          ]
        }
      ]
    }
  ]
}