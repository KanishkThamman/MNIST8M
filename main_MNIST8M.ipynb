{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# TO RUN, MAKE SURE YOU HAV ~ 9Gb of RAM AVALABLE TO USE!!!\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Enable mixed precision\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x = np.concatenate((x_train, x_test))\n",
        "y = np.concatenate((y_train, y_test))\n",
        "x = x.astype('float32') / 255.0\n",
        "\n",
        "# Move data to GPU\n",
        "x = tf.constant(x, dtype=tf.float32)\n",
        "y = tf.constant(y, dtype=tf.int32)\n",
        "\n",
        "@tf.function\n",
        "def generate_samples(images, labels, num_samples):\n",
        "    idx1 = tf.random.uniform([num_samples], 0, tf.shape(images)[0], dtype=tf.int32)\n",
        "    idx2 = tf.random.uniform([num_samples], 0, tf.shape(images)[0], dtype=tf.int32)\n",
        "\n",
        "    image1, image2 = tf.gather(images, idx1), tf.gather(images, idx2)\n",
        "    label1, label2 = tf.gather(labels, idx1), tf.gather(labels, idx2)\n",
        "\n",
        "    alpha = tf.random.uniform([num_samples, 1, 1], 0, 1)\n",
        "\n",
        "    new_images = alpha * image1 + (1 - alpha) * image2\n",
        "    new_labels = tf.cast(tf.round(alpha[:, 0, 0] * tf.cast(label1, tf.float32) + (1 - alpha[:, 0, 0]) * tf.cast(label2, tf.float32)), tf.int32)\n",
        "\n",
        "    # Random shift\n",
        "    shift = tf.random.uniform([num_samples, 2], -2, 3, dtype=tf.int32)\n",
        "    new_images = tf.map_fn(lambda x: tf.roll(x[0], x[1], [0, 1]), (new_images, shift), fn_output_signature=tf.float32)\n",
        "\n",
        "    # Random noise\n",
        "    noise = tf.random.normal(tf.shape(new_images), mean=0.0, stddev=0.05)\n",
        "    new_images = new_images + noise\n",
        "\n",
        "    new_images = tf.clip_by_value(new_images, 0, 1)\n",
        "\n",
        "    return new_images, new_labels\n",
        "\n",
        "def generate_and_save_dataset(num_samples, batch_size=8192, base_dir=\"MNIST8M\"):\n",
        "    train_dir = os.path.join(base_dir, \"train\")\n",
        "    test_dir = os.path.join(base_dir, \"test\")\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "    samples_generated = 0\n",
        "\n",
        "    for batch in tqdm(range(num_batches), desc=\"Generating and saving batches\"):\n",
        "        current_batch_size = min(batch_size, num_samples - samples_generated)\n",
        "\n",
        "        new_images, new_labels = generate_samples(x, y, current_batch_size)\n",
        "\n",
        "        # Split into train and test (70-30 split)\n",
        "        split_idx = int(0.7 * current_batch_size)\n",
        "        train_x, test_x = new_images[:split_idx], new_images[split_idx:]\n",
        "        train_y, test_y = new_labels[:split_idx], new_labels[split_idx:]\n",
        "\n",
        "        # Save train data\n",
        "        np.save(os.path.join(train_dir, f\"x_train_{batch}.npy\"), train_x.numpy())\n",
        "        np.save(os.path.join(train_dir, f\"y_train_{batch}.npy\"), train_y.numpy())\n",
        "\n",
        "        # Save test data\n",
        "        np.save(os.path.join(test_dir, f\"x_test_{batch}.npy\"), test_x.numpy())\n",
        "        np.save(os.path.join(test_dir, f\"y_test_{batch}.npy\"), test_y.numpy())\n",
        "\n",
        "        samples_generated += current_batch_size\n",
        "\n",
        "    print(\"Data generation and saving complete.\")\n",
        "    print(f\"Total samples generated: {samples_generated}\")\n",
        "\n",
        "# Generate and save the dataset\n",
        "num_samples = 8000000\n",
        "batch_size = 10000  # Adjust this based on your GPU memory\n",
        "\n",
        "generate_and_save_dataset(num_samples, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG9eQEdeMXiQ",
        "outputId": "0d4c4225-4b67-4f38-b05e-ecd1efcfb890"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available:  []\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating and saving batches: 100%|██████████| 800/800 [10:51<00:00,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data generation and saving complete.\n",
            "Total samples generated: 8000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3Oz0RXo3TEE",
        "outputId": "e8b3bc2c-a4bc-4bfe-fbe2-c845ea597e98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_generated_dataset(base_dir=\"MNIST8M\", batch_size=8000):\n",
        "    train_dir = os.path.join(base_dir, \"train\")\n",
        "    test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "    def load_data(directory, prefix):\n",
        "        files = sorted([f for f in os.listdir(directory) if f.startswith(prefix)])\n",
        "        total_samples = sum(os.path.getsize(os.path.join(directory, f)) for f in files) // (28 * 28 * 4)  # Estimate total samples\n",
        "\n",
        "        data_list = []\n",
        "        with tqdm(total=total_samples, desc=f\"Loading {prefix}\", unit=\"samples\") as pbar:\n",
        "            for file in files:\n",
        "                data = np.load(os.path.join(directory, file))\n",
        "                data_list.append(data)\n",
        "                pbar.update(len(data))\n",
        "\n",
        "        return np.concatenate(data_list)\n",
        "\n",
        "    x_train = load_data(train_dir, \"x_train\")\n",
        "    y_train = load_data(train_dir, \"y_train\")\n",
        "    x_test = load_data(test_dir, \"x_test\")\n",
        "    y_test = load_data(test_dir, \"y_test\")\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "# Load the dataset\n",
        "x_train, y_train, x_test, y_test = load_generated_dataset()\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"\\nDataset loaded successfully.\")\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhJVg4yF0XIw",
        "outputId": "2d01c13f-02b3-49ea-b93f-bd116c4dec0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading x_train:  69%|██████▊   | 3843000/5600032 [01:12<00:40, 42890.15samples/s]"
          ]
        }
      ]
    }
  ]
}